# -*- coding: utf-8 -*-
"""book_recom_Collaborative_Filtering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M6seG7rPv65NR7diHGBa3eydKsmyE0yS

<h1 align=center><font size = 6>Book Recommendation with Collaborative Filtering</font></h1>

<br>

<img src="https://img-cdn.inc.com/image/upload/w_1920,h_1080,c_fill/images/panoramic/GettyImages-577674005_492115_zfpgiw.jpg" height=520 width=1000 alt="GitHub">

<br>

<small>Picture Source: <a href="https://www.inc.com/jessica-stillman/books-reading-intelligence-tyler-cowen.html">Jessica Stillman</a>

<br>

<h2>Context</h2>

During the last few decades, with the rise of Youtube, Amazon, Netflix and many other such web services, recommender systems have taken more and more place in our lives. From e-commerce (suggest to buyers articles that could interest them) to online advertisement (suggest to users the right contents, matching their preferences), recommender systems are today unavoidable in our daily online journeys. In a very general way, recommender systems are algorithms aimed at suggesting relevant items to users (items being movies to watch, text to read, products to buy or anything else depending on industries).

<br>

Recommender systems are really critical in some industries as they can generate a huge amount of income when they are efficient or also be a way to stand out significantly from competitors. As a proof of the importance of recommender systems, we can mention that, a few years ago, Netflix organised a challenges (the “Netflix prize”) where the goal was to produce a recommender system that performs better than its own algorithm with a prize of 1 million dollars to win.

<br>

By applying this simple dataset and related tasks and notebooks , we will evolutionary go through different paradigms of recommender algorithms . For each of them, we will present how they work, describe their theoretical basis and discuss their strengths and weaknesses. For extra information, please check <a href="https://www.kaggle.com/arashnic/book-recommendation-dataset">Kaggle Möbius</a>.

<br>
<br>

<h2>Data Set</h2>

<a href="https://www.inc.com/jessica-stillman/books-reading-intelligence-tyler-cowen.html"></a>


Dataset link from Kaggle: [Book Recommendation Dataset](https://www.kaggle.com/arashnic/book-recommendation-dataset)

<br>

<h2>Objective:</h2>
<ul>
  <li>Understand the dataset.</li>
  <li>Build Pearson correlation.</li>
  <li>Make recommendations.</li>
</ul>

<br>
<h2>Keywords</h2>
<ul>
  <li>Computer Science</li>
  <li>Collaborative Filtering</li>
  <li>Pearson Correlation</li>
  <li>Recommendation Systems</li>
  <li>Book Recommendation</li>
</ul>
<br>

<h2>Content</h2>

<div class="alert alert-block alert-info" style="margin-top: 20px">

<li><a href="https://#">Importing Libraries</a></li>
<li><a href="https://#">Data Pre-processing</a></li>
<li><a href="https://#">Pearson Cerrelation for the Recommendation</a></li>
<br>

<p></p>
Estimated Time Needed: <strong>25 min</strong>
</div>

## 1. Imporing Libraries

We set the stage by importing essential libraries that will empower our exploration into personalized book recommendations. Each library plays a unique role in the analytical and mathematical aspects.

<br>

- **pandas (as pd):** A powerhouse for data manipulation and analysis, pandas will be our go-to tool for handling datasets with finesse.

- **sqrt from math:** The square root function from the math library will prove handy for certain calculations, especially when dealing with similarity metrics in collaborative filtering.

- **numpy (as np):** A fundamental library for numerical operations, numpy will be instrumental in array manipulations and mathematical computations.

- **warnings:** We're using the warnings library to suppress any distracting warning messages that might pop up during our analysis. This ensures a cleaner and more focused exploration.
"""

import pandas as pd
from math import sqrt
import numpy as np
import warnings
warnings.filterwarnings("ignore")

"""## 2. Data Pre-processing

### 2.1. Seperate Datasets

We initiate the exploration by loading and organizing the datasets that form the backbone of our book recommendation analysis.

<br>

- **books_df:** We start by loading the '**Books.csv**' dataset using pandas. This dataset encapsulates information about various books, including details such as ISBN (International Standard Book Number), book title, author, and year of publication. To streamline our analysis, we selectively choose relevant columns, including ISBN, Book-Title, Book-Author, and Year-Of-Publication.

<br>

- **ratings_df:** Simultaneously, we import the '**Ratings.csv**' dataset, a crucial component for collaborative filtering. This dataset contains user ratings for various books, forming the basis for understanding user preferences and generating personalized recommendations.
"""

books_df = pd.read_csv('Books.csv')

books_df = books_df[['ISBN', 'Book-Title', 'Book-Author', 'Year-Of-Publication']]
ratings_df = pd.read_csv('Ratings.csv')

books_df.head()

"""### 2.2. Looking for Duplicated Book Titles

We conduct a critical examination of the '**Book-Title**' column in the '**books_df**' dataset to identify and handle any instances of duplicated book titles.
"""

books_df['Book-Title'].duplicated().sum()

books_df.drop_duplicates(subset='Book-Title', keep="last", inplace=True)

books_df['Book-Title'].duplicated().sum()

ratings_df.head()

"""### 2.3. User Input for Making Recommendations

Now, we need to define our books for making recommendations. Otherwise, we can't clearly make recommandations without user input.
"""

userInput = [
            {'Book-Title': 'Lightning', 'Book-Rating': 9},
            {'Book-Title': 'Manhattan Hunt Club', 'Book-Rating': 9},
            {'Book-Title': 'Clara Callan', 'Book-Rating': 7},
            {'Book-Title': "Jane Doe", 'Book-Rating': 2},
            {'Book-Title': 'Wild Animus', 'Book-Rating': 5}
         ]
inputBooks= pd.DataFrame(userInput)
inputBooks

"""### 2.4. Creating User Subset for Collaborative Filtering

The focus is on preparing the input data for the collaborative filtering analysis. The process begins by identifying the subset of the '**books_df**' dataset that corresponds to the book titles specified in the user's input. This is achieved through the use of the `.isin()` method, allowing the extraction of relevant rows based on the 'Book-Title' column from the user's input. Subsequently, a merging operation takes place to obtain the necessary details, including '**ISBN**', for the identified books. This merging step is crucial for aligning the user's input with the broader dataset.

Following the merge, unnecessary information is trimmed down by dropping the '**Year-Of-Publication**' column from the '**inputBooks**' dataframe. The final result is a refined input dataframe that encapsulates the essential details of the user-specified books. It's worth noting that if a book specified by the user is not present in this final input dataframe, it may be due to variations in spelling or capitalization, warranting a careful check for data consistency. This meticulous preparation ensures that the input data is well-structured and ready for integration into the collaborative filtering model, laying the foundation for generating personalized book recommendations based on user preferences.
"""

inputId = books_df[books_df['Book-Title'].isin(inputBooks['Book-Title'].tolist())]
inputBooks = pd.merge(inputId, inputBooks)
inputBooks = inputBooks.drop('Year-Of-Publication', 1)
inputBooks

"""We had merged our recommendation dataframe and the ISBN number of the books with book's author."""

# Filtering out users that have watched movies that the input has watched and storing it
userSubset = ratings_df[ratings_df['ISBN'].isin(inputBooks['ISBN'].tolist())]
userSubset.head()

"""The focus shifts to creating a user subset for collaborative filtering analysis. The user subset is generated by grouping the '**userSubset**' dataframe based on the '**User-ID**' column. A specific user, in this case, '**User-ID**' 18401, is isolated for closer inspection using the `get_group()` method."""

userSubsetGroup = userSubset.groupby(['User-ID'])

userSubsetGroup.get_group(18401)

"""The user subset is then organized by the number of entries per user in descending order, achieved through sorting the '**userSubsetGroup**' using a lambda function. This arrangement allows prioritizing users with a higher number of interactions, contributing to a more robust collaborative filtering model."""

userSubsetGroup = sorted(userSubsetGroup,  key=lambda x: len(x[1]), reverse=True)

userSubsetGroup[0:3]

"""To manage computational resources effectively, the user subset is further narrowed down to the top 100 users. This selection, stored in 'userSubsetGroup,' represents a subset of users with substantial engagement, forming the basis for collaborative filtering analysis. This strategic curation of the user subset ensures that the collaborative filtering model is not only computationally efficient but also prioritizes users with a significant impact on recommendations."""

userSubsetGroup = userSubsetGroup[0:100]

"""## 3. Pearson Correlation for the Recommendation

The Pearson correlation coefficient (ρ) is a statistical measure that quantifies the linear relationship between two variables, X and Y. The formula for calculating Pearson correlation is as follows:

<br>

$$ \rho = \frac{\sum{(X_i - \bar{X})(Y_i - \bar{Y})}}{\sqrt{\sum{(X_i - \bar{X})^2} \sum{(Y_i - \bar{Y})^2}}} $$

<br>

Here's a breakdown of the terms in the formula:

- $ \rho $: Pearson correlation coefficient.
- $ X_i $ and $ Y_i $: Individual data points in the datasets X and Y.
- $ \bar{X} $ and $ \bar{Y} $: Mean (average) of the respective datasets X and Y.

The numerator represents the sum of the product of the differences between each data point and the mean of its respective dataset. The denominator involves the square root of the product of the sums of squared differences from the mean for both datasets.

<br>

The resulting Pearson correlation coefficient ranges from -1 to 1:

- $ \rho = 1 $: Perfect positive correlation.
- $ \rho = -1 $: Perfect negative correlation.
- $ \rho = 0 $: No linear correlation.

<br>

In collaborative filtering for book recommendations, Pearson correlation is commonly used to measure the similarity between user preferences based on their ratings. A positive correlation suggests similar tastes, while a negative correlation implies dissimilar preferences.
"""

# Initialize an empty dictionary to store Pearson correlation coefficients
pearsonCorrelationDict = {}

# For every user group in our subset
for name, group in userSubsetGroup:
    # Sort the input and current user group by ISBN for consistency
    group = group.sort_values(by='ISBN')
    inputBooks = inputBooks.sort_values(by='ISBN')

    # Get the number of ratings (N) for the formula
    nRatings = len(group)

    # Get the review scores for the books they both have in common
    temp_df = inputBooks[inputBooks['ISBN'].isin(group['ISBN'].tolist())]

    # Store review scores in temporary lists for future calculations
    tempRatingList = temp_df['Book-Rating'].tolist()
    tempGroupList = group['Book-Rating'].tolist()

    # Calculate the components of the Pearson correlation formula
    Sxx = sum([i**2 for i in tempRatingList]) - pow(sum(tempRatingList),2)/float(nRatings)
    Syy = sum([i**2 for i in tempGroupList]) - pow(sum(tempGroupList),2)/float(nRatings)
    Sxy = sum(i*j for i, j in zip(tempRatingList, tempGroupList)) - sum(tempRatingList)*sum(tempGroupList)/float(nRatings)

    # If the denominator is different than zero, then calculate Pearson correlation, else, set correlation to 0
    if Sxx != 0 and Syy != 0:
        pearsonCorrelationDict[name] = Sxy/sqrt(Sxx*Syy)
    else:
        pearsonCorrelationDict[name] = 0

"""The resulting dictionary '**pearsonCorrelationDict**' contains Pearson correlation coefficients between the input user and other users in the subset. This code calculates how similar the preferences of the input user are to each user in the subset."""

pearsonCorrelationDict.items()

"""### 3.1. Pearson Correlation Dictonary on DataFrame

We codded transforms the calculated Pearson correlation coefficients into a structured DataFrame ('**pearsonDF**'), where each row corresponds to a user in the subset, and columns include the similarity index and user ID. This DataFrame is a valuable resource for further analysis and recommendation generation in collaborative filtering.
"""

pearsonDF = pd.DataFrame.from_dict(pearsonCorrelationDict, orient='index')
pearsonDF.columns = ['similarityIndex']
pearsonDF['userId'] = pearsonDF.index
pearsonDF.index = range(len(pearsonDF))
pearsonDF.head()

"""### 3.2. Extracting Top Similar Users

The provided code segment involves extracting the top users from the '**pearsonDF**' DataFrame, sorting them based on their similarity indices in descending order, and displaying the first few rows.
"""

topUsers=pearsonDF.sort_values(by='similarityIndex', ascending=False)[0:50]
topUsers.head()

"""### 3.3. Merging Top Similar Users with Ratings Data

The code segment involves merging the '**topUsers**' DataFrame with the '**ratings_df**' DataFrame based on user IDs and displaying the first few rows of the resulting DataFrame.
"""

topUsersRating=topUsers.merge(ratings_df, left_on='userId', right_on='User-ID', how='inner')
topUsersRating.head()

"""### 3.4. Calculating Weighted Ratings for Top Similar Users

It involves calculating weighted ratings for the top similar users by multiplying their similarity indices with their respective book ratings.
"""

topUsersRating['weightedRating'] = topUsersRating['similarityIndex']*topUsersRating['Book-Rating']
topUsersRating.head()

"""### 3.5. Aggregating Weighted Ratings for Books

Aggregating the weighted ratings and similarity indices for each book in the '**topUsersRating**' DataFrame.
"""

tempTopUsersRating = topUsersRating.groupby('ISBN').sum()[['similarityIndex','weightedRating']]
tempTopUsersRating.columns = ['sum_similarityIndex','sum_weightedRating']
tempTopUsersRating.head()

"""### 3.6. Generating Weighted Average Recommendation Scores

Calculating the weighted average recommendation scores for books based on the aggregated information from the collaborative filtering process.
"""

recommendation_df = pd.DataFrame()
# Now we take the weighted average
recommendation_df['weighted average recommendation score'] = tempTopUsersRating['sum_weightedRating']/tempTopUsersRating['sum_similarityIndex']
recommendation_df['ISBN'] = tempTopUsersRating.index
recommendation_df.head()

"""### 3.7. Sorting Books by Weighted Average Recommendation Scores

Sorting the '**recommendation_df**' DataFrame based on the calculated weighted average recommendation scores in descending order.
"""

recommendation_df = recommendation_df.sort_values(by='weighted average recommendation score', ascending=False)
recommendation_df.head()

"""### 3.8. Retrieving Book Details for Top Recommendations

Retrieving detailed information about the top recommended books by matching their ISBN values with the '**books_df**' DataFrame.
"""

books_df.loc[books_df['ISBN'].isin(recommendation_df.head(10)['ISBN'].tolist())]

"""The code efficiently identifies and displays detailed information about the top 10 recommended books by matching their ISBN values with the '**books_df**' DataFrame. This final output provides a comprehensive view of the recommended books, including their titles, authors, and other relevant details.

<h1>Contact Me</h1>
<p>If you have something to say to me please contact me:</p>

<ul>
  <li>Twitter: <a href="https://twitter.com/Doguilmak">Doguilmak</a></li>
  <li>Mail address: doguilmak@gmail.com</li>
</ul>
"""

from datetime import datetime
print(f"Changes have been made to the project on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")